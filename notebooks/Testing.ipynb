{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f48a8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imports\n",
    "import math\n",
    "import scipy.ndimage\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8af448f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utils\n",
    "\n",
    "def first(aList):\n",
    "    return aList[0]\n",
    "\n",
    "def last(aList):\n",
    "    return aList[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1bfbc679",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setting parameters\n",
    "\n",
    "IMAGE_1 = '../images/Image 1a.png'\n",
    "IMAGE_2 = '../images/Image 1b.png'\n",
    "FRAMES_PER_IMAGE = 1\n",
    "\n",
    "DEFAULT_PROOF = 0\n",
    "DEFAULT_OVERLAP = 50\n",
    "DEFAULT_INTERROGATION_WINDOW = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d392c4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reading images\n",
    "# Loading images as an IxJ matrix, containing the intensity of each pixel.\n",
    "#\n",
    "# Output: \n",
    "# Array with the following dimensions: 0 - Image; 1 - Height (Y); 2 - Width (X).\n",
    "\n",
    "def load_images():\n",
    "    images = []\n",
    "    \n",
    "    for IMAGE in [IMAGE_1, IMAGE_2]:\n",
    "        imag = Image.open(IMAGE)\n",
    "        grayscale_image = imag.convert(\"L\")\n",
    "        grayscale_array = np.asarray(grayscale_image)\n",
    "        images += [np.array(grayscale_array)]\n",
    "    \n",
    "    return np.array(images)\n",
    "\n",
    "def load_fake_images(x=100, y=None, total_images=5, mode='const'):\n",
    "    if not y:\n",
    "        y = x\n",
    "    count = 1\n",
    "    images = []\n",
    "    for idx in range(total_images):\n",
    "        if mode == 'rand':\n",
    "            images += [(np.random.rand(x, y) * 100).astype(np.uint8)]\n",
    "        elif mode == 'inc':\n",
    "            images += [np.reshape(np.arange(count, count + x * y), [x, y])]\n",
    "            count += x * y\n",
    "        else:\n",
    "            images += [np.ones((x, y), np.uint8) * (idx + 1)]\n",
    "    return np.array(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d06f587",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Single to double frame\n",
    "# Combines images by 2, returning an array with two frames (one for each image). \n",
    "#\n",
    "#   Input: 5 images with step 1.\n",
    "#   Output: 4 double-framed images.\n",
    "#      FrameA:  1  2  3  4\n",
    "#      FrameB:  2  3  4  5\n",
    "#\n",
    "#   Input: 8 images with step 3.\n",
    "#   Output: 5 doubled-framed images.\n",
    "#      FrameA:  1  2  3  4  5\n",
    "#      FrameB:  4  5  6  7  8\n",
    "#\n",
    "# This function also crops the image according to the provided Region of Interest (ROI), that must be passed as:\n",
    "# ROI = [X-start X-end Y-start Y-end], for example: [1 100 1 50].\n",
    "#\n",
    "# Output:\n",
    "# Array with the following dimensions: 0 - Image; 1 - Frame; 2 - Height (Y); 3 - Width (X).\n",
    "\n",
    "def single_to_double_frame(images, step=1, roi=None):\n",
    "    total_images = len(images)\n",
    "\n",
    "    frameA_idx = list(range(0,total_images-step))\n",
    "    frameB_idx = [idx+1 for idx in frameA_idx]\n",
    "\n",
    "    height, width = first(images).shape\n",
    "    mask = np.ones([height, width], np.uint8)\n",
    "\n",
    "    images_double_framed = []\n",
    "    for idx in frameA_idx:\n",
    "        double_frame = (images[frameA_idx[idx]], images[frameB_idx[idx]])\n",
    "            \n",
    "        if roi and len(roi) == 4:\n",
    "            size_y, size_x = double_frame[0].shape\n",
    "            min_x, max_x = max(0, roi[0]-1), min(roi[1], size_x)\n",
    "            min_y, max_y = max(0, roi[2]-1), min(roi[3], size_x)\n",
    "            \n",
    "            double_frame[0] = np.array(double_frame[0][min_y:max_y, min_x:max_x])\n",
    "            double_frame[1] = np.array(double_frame[1][min_y:max_y, min_x:max_x])\n",
    "\n",
    "        images_double_framed += [double_frame]\n",
    "            \n",
    "    return np.array(images_double_framed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b8dbbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculate phases\n",
    "# Number of phases to reconstruct. If set to 1, no phase reconstruction is achieved.\n",
    "#\n",
    "# Output:\n",
    "# A plain number representing the different phases and and a vector of phases.\n",
    "\n",
    "def calculate_phases(images, cumulcross=True, acquisition_freq=1, actuation_freq=1, default_phases=1):\n",
    "    if cumulcross:\n",
    "        T_acquired = np.array(range(len(images))) / acquisition_freq\n",
    "        \n",
    "        if actuation_freq == 0:\n",
    "            phases = T_acquired * 0 + 1\n",
    "            total_phases = 1\n",
    "        else:\n",
    "            phases = np.floor(np.mod(T_acquired + 1 / (2 * actuation_freq), 1 / actuation_freq) * actuation_freq * default_phases) + 1\n",
    "            total_phases = default_phases\n",
    "    else:\n",
    "        phases = np.array(range(len(images)))\n",
    "        total_phases = len(phases)\n",
    "        \n",
    "    return total_phases, phases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67afe591",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prepare images for PIV\n",
    "# Determine which indices must be used to create the interrogation windows. \n",
    "# It also add a padding dark color to the images.\n",
    "#\n",
    "# Output: Indexes for vectors (MinX, MaxX, MinY, MaxY), the padded images and the interrogation window indexes.\n",
    "\n",
    "def prepare_piv_images(images, window_size, step):\n",
    "    \n",
    "    # Calculating vectors.\n",
    "    min_x = 1 + math.ceil(step) # TODO: Check if shouldn't be 0 + step (because of Python vs Matlab start index).\n",
    "    min_y = 1 + math.ceil(step) # TODO: Check if shouldn't be 0 + step (because of Python vs Matlab start index).\n",
    "    size_y, size_x = first(images)[0].shape\n",
    "    max_x = step * math.floor(size_x / step) - (window_size - 1) + math.ceil(step)\n",
    "    max_y = step * math.floor(size_y / step) - (window_size - 1) + math.ceil(step)\n",
    "    vectors_u = math.floor((max_x - min_x)/step + 1)\n",
    "    vectors_v = math.floor((max_y - min_y)/step + 1)\n",
    "    \n",
    "    # Centering image grid.\n",
    "    pad_x = size_x - max_x\n",
    "    pad_y = size_y - max_y\n",
    "    shift_x = max(0, round((pad_x - min_x) / 2))\n",
    "    shift_y = max(0, round((pad_y - min_y) / 2))\n",
    "    min_x += shift_x\n",
    "    min_y += shift_y\n",
    "    max_x += shift_x\n",
    "    max_y += shift_y\n",
    "    \n",
    "    # Adding a dark padded border to images.\n",
    "    padded_images = []\n",
    "    for idx in range(len(images)):\n",
    "        padded_images += [[]]\n",
    "        for frame in range(2):\n",
    "            image = images[idx][frame]\n",
    "            padded_images[idx] += [np.pad(image, math.ceil(window_size-step), constant_values=image.min())]\n",
    "        padded_images[idx] = np.array(padded_images[idx])\n",
    "    padded_images = np.array(padded_images)\n",
    "    \n",
    "    # Interrogation window indexes for first frame.\n",
    "    padded_size_y, padded_size_x = first(padded_images)[0].shape\n",
    "    min_s0 = np.repmat(np.array(np.arange(min_y, max_y + 1, step) - 1).transpose(), 1, vectors_u)\n",
    "    max_s0 = np.repmat(np.array(np.arange(min_x, max_x + 1, step) - 1) * padded_size_y, vectors_v, 1)\n",
    "    s0 = np.asarray(min_s0 + max_s0).flatten()[..., np.newaxis, np.newaxis].transpose([1, 2, 0])\n",
    "\n",
    "    min_s1 = np.repmat(np.array(np.arange(1, window_size + 1)).transpose(), 1, window_size)\n",
    "    max_s1 = np.repmat(np.array(np.arange(1, window_size + 1) - 1) * padded_size_y, window_size, 1)\n",
    "    s1 = min_s1 + max_s1\n",
    "\n",
    "    indexes = np.tile(np.asarray(s1)[..., np.newaxis], [1, 1, s0.shape[2]]) + np.tile(s0, [window_size, window_size, 1]) - 1\n",
    "    \n",
    "    return min_x, max_x, min_y, max_y, padded_images, indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a90487a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cumulative cross correlation\n",
    "# Averages correlation maps from an image stack.\n",
    "#\n",
    "# TODO: This function isn't working properly! Matlab FFT â‰  Numpy FFT.\n",
    "# Should fix the cross correlation calculation and also check the normalization (different shape expected).\n",
    "#\n",
    "# Output: A correlation matrix with the same size as the images input.\n",
    "\n",
    "NORMALIZED_CORRELATION_RESOLUTION = 2**8\n",
    "def cumulative_cross_correlation(images, indexes, int_window):\n",
    "    \n",
    "    total_correlation = 0\n",
    "    for idx, image in enumerate(images):\n",
    "        frame_a = image[0].take(indexes).astype(np.single)\n",
    "        frame_b = image[1].take(indexes).astype(np.single)\n",
    "        \n",
    "        # Calculating cross correlation\n",
    "        fft_a = np.fft.fft2(frame_a)\n",
    "        fft_b = np.fft.fft2(frame_b)\n",
    "\n",
    "        fft_shifting = np.real(np.fft.ifft(np.fft.ifft(np.conj(fft_a) * fft_b, window_size, 2), window_size, 1))\n",
    "        correlation = np.fft.fftshift(np.fft.fftshift(fft_shifting, 2), 1)\n",
    "        correlation[correlation < 0] = 0\n",
    "        \n",
    "        # Normalizing correlation\n",
    "        min_corr = np.tile(correlation.min(0).min(0), [correlation.shape[0], correlation.shape[1], 1])\n",
    "        max_corr = np.tile(correlation.max(0).max(0), [correlation.shape[0], correlation.shape[1], 1])\n",
    "        norm_corr = (correlation - min_corr) / (max_corr - min_corr) * (NORMALIZED_CORRELATION_RESOLUTION - 1)\n",
    "    \n",
    "        total_correlation += norm_corr/len(images)\n",
    "        \n",
    "    return total_correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3cec7b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Vector field determination\n",
    "# No fucking clue what this does. Here it's where it happens the magic, calculating peaks and doing science shit.\n",
    "#\n",
    "# Output: OutputPIV object\n",
    "\n",
    "S2N_FILTER = False\n",
    "DEFAULT_S2N_THRESHOLD = 1\n",
    "DEFAULT_RES_NORMALIZATION = 255\n",
    "def vector_field_determination(correlation, int_window, step, min_x, max_x, min_y, max_y):\n",
    "    \n",
    "    # Normalize result\n",
    "    squeezed_min_corr = correlation.min(0).min(0).squeeze()[:, np.newaxis, np.newaxis]\n",
    "    squeezed_delta_corr = correlation.max(0).max(0).squeeze()[:, np.newaxis, np.newaxis] - squeezed_min_corr\n",
    "    min_res = np.tile(squeezed_min_corr, [1, correlation.shape[0], correlation.shape[1]]).transpose([1, 2, 0])\n",
    "    delta_res = np.tile(squeezed_delta_corr, [1, correlation.shape[0], correlation.shape[1]]).transpose([1, 2, 0])\n",
    "    corr = ((correlation - min_res) / delta_res) * DEFAULT_RES_NORMALIZATION\n",
    "    \n",
    "    # Find peaks and S2N\n",
    "    x1, y1, indexes1, x2, y2, indexes2, s2n = find_all_displacements(corr)\n",
    "    \n",
    "    # Sub-pixel determination\n",
    "    pixel_offset = 1 if (int_window % 2 == 0) else 0.5\n",
    "    vector = sub_pixel_gaussian(corr, int_window, x1, y1, indexes1, pixel_offset)\n",
    "    \n",
    "    # Create data\n",
    "    x_range = np.arange(min_x, max_x, step)\n",
    "    y_range = np.arange(min_y, max_y, step)\n",
    "    output_x = np.tile(x_range + int_window / 2, [len(y_range), 1])\n",
    "    output_y = np.tile(y_range[:, None] + int_window / 2, [1, len(x_range)])\n",
    "\n",
    "    # TODO: Check flow, while testing vector had not enough elements to reshape.\n",
    "    vector = np.reshape(vector, np.append(np.array(output_x.transpose().shape), 2), order='F').transpose([1, 0, 2])\n",
    "\n",
    "    # Signal to noise filter\n",
    "    s2n = s2n[np.reshape(np.array(range(output_x.size)), output_x.transpose().shape, order='F').transpose()]\n",
    "    if S2N_FILTER:\n",
    "        vector[:,:,0] = vector[:,:,0] * (s2n > DEFAULT_S2N_THRESHOLD)\n",
    "        vector[:,:,1] = vector[:,:,1] * (s2n > DEFAULT_S2N_THRESHOLD)\n",
    "    \n",
    "    output_u = vector[:,:,0]\n",
    "    output_v = vector[:,:,1]\n",
    "\n",
    "    output_x -= int_window/2\n",
    "    output_y -= int_window/2\n",
    "\n",
    "    return OutputPIV(output_x, output_y, output_u, output_v, s2n)\n",
    "    \n",
    "    \n",
    "## Gaussian sub-pixel mode\n",
    "# No f*cking clue what this does.\n",
    "#\n",
    "# Output: A vector with a sub-pixel deviation - Maybe? I'm not sure. Its dimensions are Number-of-Correlations by 2. \n",
    "\n",
    "def sub_pixel_gaussian(correlation, int_window, x, y, indexes, pixel_offset):\n",
    "    z = np.array(range(indexes.shape[0])).transpose()\n",
    "    \n",
    "    xi = np.nonzero(np.logical_not(np.logical_and(\n",
    "        # Adjusting -1 to -2 according to Matlab/Python mapping.\n",
    "        np.logical_and(x <= correlation.shape[1] - 2, y <= correlation.shape[0] - 2),\n",
    "        np.logical_and(x >= 2, y >= 2)\n",
    "    )))[0]\n",
    "\n",
    "    x = np.delete(x, xi)\n",
    "    y = np.delete(y, xi)\n",
    "    z = np.delete(z, xi)\n",
    "    x_max = correlation.shape[1]\n",
    "    vector = np.ones((correlation.shape[2], 2)) * np.nan\n",
    "\n",
    "    if len(x) > 0:\n",
    "        ip = np.ravel_multi_index(np.array([x, y, z]), correlation.shape, order='F')\n",
    "        flattened_correlation = correlation.flatten(order='F')\n",
    "\n",
    "        f0 = np.log(flattened_correlation[ip])\n",
    "        f1 = np.log(flattened_correlation[ip - 1])\n",
    "        f2 = np.log(flattened_correlation[ip + 1])\n",
    "        peak_y = y + (f1 - f2) / (2 * f1 - 4 * f0 + 2 * f2)\n",
    "\n",
    "        f1 = np.log(flattened_correlation[ip - x_max])\n",
    "        f2 = np.log(flattened_correlation[ip + x_max])\n",
    "        peak_x = y + (f1 - f2) / (2 * f1 - 4 * f0 + 2 * f2)\n",
    "    \n",
    "        sub_pixel_x = peak_x - (int_window / 2) - pixel_offset\n",
    "        sub_pixel_y = peak_y - (int_window / 2) - pixel_offset\n",
    "    \n",
    "        vector[z, :] = np.array([sub_pixel_x, sub_pixel_y]).transpose()\n",
    "    \n",
    "    return vector\n",
    "\n",
    "    \n",
    "## Find all displacements\n",
    "# Find all integer pixel displacement in a stack of correlation windows.\n",
    "#\n",
    "# Output: Horizontal and vertical indexes of the first and second maximum for each slice of correlation in the third\n",
    "# dimension (PeakX1, PeackY1, PeakX2, PeakY2), the absolute indexes of the correlation maximums (Idx1, Idx2) and the\n",
    "# ratio between the first and second peack (S2N) - 0 indicates non confiable results.\n",
    "\n",
    "def find_all_displacements(correlation):\n",
    "    corr_size = correlation.shape[0]\n",
    "    \n",
    "    # Finding first peak\n",
    "    peak1_val, peak1_x, peak1_y, peak_indexes1, peak_positions1 = find_peaks(correlation)\n",
    "\n",
    "    # Finding second peak (1 extra point from Matlab size)\n",
    "    filter_size = 10 if corr_size >= 64 else 5 if corr_size >= 32 else 4\n",
    "    filtered = scipy.ndimage.correlate(peak_positions1, np.ones([filter_size, filter_size, 1]), mode='constant')\n",
    "    correlation = (1 - filtered) * correlation\n",
    "    peak2_val, peak2_x, peak2_y, peak_indexes2, _ = find_peaks(correlation)\n",
    "\n",
    "    # Calculating Signal to Noise ratio\n",
    "    signal_to_noise = np.zeros([peak1_val.shape[0]])\n",
    "    signal_to_noise[peak2_val != 0] = peak1_val[peak2_val != 0] / peak2_val[peak2_val != 0]\n",
    "\n",
    "    # Maximum at a border usually indicates that MAX took the first one it found, so we should put a bad S2N, like 0.\n",
    "    signal_to_noise[peak1_y == 0] = 0\n",
    "    signal_to_noise[peak1_x == 0] = 0\n",
    "    signal_to_noise[peak1_y == (corr_size - 1)] = 0\n",
    "    signal_to_noise[peak1_x == (corr_size - 1)] = 0\n",
    "    signal_to_noise[peak2_y == 0] = 0\n",
    "    signal_to_noise[peak2_x == 0] = 0\n",
    "    signal_to_noise[peak2_y == (corr_size - 1)] = 0\n",
    "    signal_to_noise[peak2_x == (corr_size - 1)] = 0\n",
    "    \n",
    "    return peak1_x, peak1_y, peak_indexes2, peak2_x, peak2_y, peak_indexes2, signal_to_noise\n",
    "    \n",
    "    \n",
    "## Find peaks\n",
    "# Find max values for each correlation.\n",
    "#\n",
    "# Output: The MAX peak, its coordinates (X and Y) and the indexes.\n",
    "    \n",
    "def find_peaks(correlation):\n",
    "    corr_size = correlation.shape[0]\n",
    "    corr_numbers = correlation.shape[2]\n",
    "    max_peak = correlation.max(0).max(0)\n",
    "    max_positions = correlation == np.tile(max_peak[np.newaxis, np.newaxis, ...], [corr_size, corr_size, 1])\n",
    "    max_indexes = np.where(max_positions.transpose(2, 1, 0).flatten())[0]\n",
    "    peak_y, peak_x, peak_z = np.unravel_index(max_indexes, (corr_size, corr_size, corr_numbers), order='F')\n",
    "\n",
    "    # If two elements equals to the max we should check if they are in the same layer and take the first one.\n",
    "    # Surely the second one will be the second highest peak. Anyway this would be a bad vector.\n",
    "    unique_max_indexes = np.unique(peak_z)\n",
    "    max_indexes = max_indexes[unique_max_indexes]\n",
    "    peak_x = peak_x[unique_max_indexes]\n",
    "    peak_y = peak_y[unique_max_indexes]\n",
    "    \n",
    "    return max_peak, peak_x, peak_y, max_indexes, max_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fead6257",
   "metadata": {},
   "outputs": [],
   "source": [
    "## PIV Output data model\n",
    "# An ad-hoc object with the following fields: X, Y, U (X velocity), V (Y velocity) and S2N (signal to noise ratio).\n",
    "\n",
    "class OutputPIV:\n",
    "    def __init__(self, x, y, u, v, s2n):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.u = u\n",
    "        self.v = v\n",
    "        self.s2n = s2n    \n",
    "    \n",
    "\n",
    "## Calculate PIV\n",
    "# Generate the PIV data from the images loaded with the input parameters.\n",
    "#\n",
    "# Output: OutputPIV object\n",
    "\n",
    "def PIV(images, int_window, overlap=DEFAULT_OVERLAP):\n",
    "    step = round(int_window * overlap / 100)\n",
    "    min_x, max_x, min_y, max_y, padded_images, indexes = prepare_piv_images(images, int_window, step)\n",
    "    correlation = cumulative_cross_correlation(padded_images, indexes, int_window)\n",
    "    data = vector_field_determination(correlation, int_window, step, min_x, max_x, min_y, max_y)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "115f0737",
   "metadata": {},
   "source": [
    "int_window = DEFAULT_INTERROGATION_WINDOW\n",
    "\n",
    "# images = load_images()\n",
    "input_images = load_fake_images()\n",
    "double_framed_images = single_to_double_frame(input_images)\n",
    "total_phases, phases_idxs = calculate_phases(double_framed_images)\n",
    "\n",
    "for phase in range(total_phases+1):\n",
    "    images = double_framed_images[phases_idxs == phase]\n",
    "    \n",
    "    if images.size != 0:\n",
    "        parameters = InputPIV(int_window=DEFAULT_INTERROGATION_WINDOW)\n",
    "        piv_data = PIV(images, int_window)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c998b491",
   "metadata": {},
   "source": [
    "# Creating vector for sub_pixel_gaussian\n",
    "vector = np.ones([30,2]) * np.nan\n",
    "vector[0,:] = [45.9987, 10388.9978]\n",
    "vector[1,:] = [77.9862, 2061.9627]\n",
    "vector[2,:] = [109.9972, 3085.9812]\n",
    "vector[3,:] = [141.9863, 4109.9623]\n",
    "vector[4,:] = [173.9624, 5133.9961]\n",
    "vector[5,:] = [205.9962, 6157.9845]\n",
    "vector[6,:] = [237.9654, 7181.9717]\n",
    "vector[7,:] = [259.9874, 8205.9978]\n",
    "vector[8,:] = [301.9912, 9229.9888]\n",
    "vector[9,:] = [333.9768, 10253.9909]\n",
    "vector[10,:] = [365.9659, 11277.9854]\n",
    "vector[11,:] = [397.9991, 12301.9979]\n",
    "vector[12,:] = [429.9702, 13325.9602]\n",
    "vector[13,:] = [441.9812, 14349.9865]\n",
    "vector[14,:] = [493.9911, 15373.9970]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
