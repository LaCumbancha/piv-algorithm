{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e348fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Notebook utils\n",
    "\n",
    "import scipy.io\n",
    "\n",
    "def export_to_matlab(output_file, data):\n",
    "    scipy.io.savemat(output_file, mdict={'data': data})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07888230",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Starting Octave client\n",
    "\n",
    "from oct2py import Oct2Py\n",
    "\n",
    "octave_cli = Oct2Py()\n",
    "octave_cli.addpath('../../matlab/');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed00fcec",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Communication Input data model\n",
    "# An ad-hoc object with the images to analyze, the points and the algorithm settings. \n",
    "# Points: Dictionary with the Point ID as key and a ad-hoc object with PositionX, PositionY and a list of the two images (as PIL.Image.Image) as value.\n",
    "# Settings.TimeDelta: Time between two images, iin miliseconds.\n",
    "# Settings.Scale: Image scaling, in pixels per milimeters.\n",
    "# Settings.WindowSize: Interrogation Window size, default is 32.\n",
    "# Settings.RoiSize: Region of Interest size, default is None which will be used as the full image.\n",
    "\n",
    "class InputPIV:\n",
    "    def __init__(self, points, time_delta, scale, window_size=32, roi_size=None):\n",
    "        self.points = points\n",
    "        self.settings = Settings(time_delta, scale, window_size, roi_size)\n",
    "        \n",
    "\n",
    "class Settings:\n",
    "    def __init__(self, time_delta, scale, window_size, roi_size):\n",
    "        self.time_delta = time_delta\n",
    "        self.scale = scale\n",
    "        self.window_size = window_size\n",
    "        self.roi_size = roi_size\n",
    "        \n",
    "\n",
    "class Point:\n",
    "    def __init__(self, pos_x, pos_y, images):\n",
    "        self.pos_x = pos_x\n",
    "        self.pos_y = pos_y\n",
    "        self.images = images\n",
    "\n",
    "\n",
    "## Communication Output data model\n",
    "# An ad-hoc object with the following fields: X, Y, U (X velocity), V (Y velocity) and S2N (signal to noise ratio).\n",
    "\n",
    "class OutputPIV:\n",
    "    def __init__(self, x, y, u, v, s2n):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.u = u\n",
    "        self.v = v\n",
    "        self.s2n = s2n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8af448f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utils\n",
    "\n",
    "def first(a_list):\n",
    "    return a_list[0]\n",
    "\n",
    "def last(a_list):\n",
    "    return a_list[-1]\n",
    "\n",
    "def group_by(a_list, a_func=lambda x: x):\n",
    "    output_dict = {}\n",
    "    \n",
    "    for a_elem in a_list:\n",
    "        key = a_func(a_elem)\n",
    "        output_dict[key] = output_dict.get(key, []) + [a_elem]\n",
    "        \n",
    "    return list(output_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d392c4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Externals\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "## Reading images\n",
    "# Loading images as an IxJ matrix, containing the intensity of each pixel.\n",
    "#\n",
    "# Output: \n",
    "# Array with the following dimensions: 0 - Image; 1 - Height (Y); 2 - Width (X).\n",
    "\n",
    "IMAGE_1 = '../images/Image 1a.png'\n",
    "IMAGE_2 = '../images/Image 1b.png'\n",
    "\n",
    "def load_images(images_paths=[IMAGE_1, IMAGE_2]):\n",
    "    images = []\n",
    "    \n",
    "    for image in images_paths:\n",
    "        img = Image.open(image)\n",
    "        grayscale_image = img.convert(\"L\")\n",
    "        grayscale_array = np.asarray(grayscale_image)\n",
    "        images += [np.array(grayscale_array)]\n",
    "    \n",
    "    return np.array(images)\n",
    "\n",
    "\n",
    "def load_fake_images(y=100, x=None, total_images=5, mode='const'):\n",
    "    if not x:\n",
    "        x = y\n",
    "        \n",
    "    count = 1\n",
    "    images = []\n",
    "    for idx in range(total_images):\n",
    "        if mode == 'rand':\n",
    "            images += [(np.random.rand(y, x) * 100).astype(np.uint8)]\n",
    "        elif mode == 'inc':\n",
    "            images += [np.reshape(np.arange(count, count + y * x), [y, x], order='F')]\n",
    "            count += y * x\n",
    "        else:\n",
    "            images += [np.ones((y, x), np.uint8) * (idx + 1)]\n",
    "    return np.array(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d06f587",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Externals\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "## Single to double frame\n",
    "# Combines images by 2, returning an array with two frames (one for each image). \n",
    "#\n",
    "#   Input: 5 images with step 1.\n",
    "#   Output: 4 double-framed images.\n",
    "#      FrameA:  1  2  3  4\n",
    "#      FrameB:  2  3  4  5\n",
    "#\n",
    "#   Input: 8 images with step 3.\n",
    "#   Output: 5 doubled-framed images.\n",
    "#      FrameA:  1  2  3  4  5\n",
    "#      FrameB:  4  5  6  7  8\n",
    "#\n",
    "# This function also crops the image according to the provided Region of Interest (ROI), that must be passed as:\n",
    "# ROI = [X-start X-end Y-start Y-end], for example: [1 100 1 50].\n",
    "#\n",
    "# Output:\n",
    "# Array with the following dimensions: 0 - Image; 1 - Frame; 2 - Height (Y); 3 - Width (X).\n",
    "\n",
    "def single_to_double_frame(images, step=1, roi=None):\n",
    "    total_images = images.shape[0]\n",
    "\n",
    "    frameA_idx = list(range(0,total_images-step))\n",
    "    frameB_idx = [idx+1 for idx in frameA_idx]\n",
    "\n",
    "    images_double_framed = []\n",
    "    for idx in frameA_idx:\n",
    "        double_frame = [images[frameA_idx[idx]], images[frameB_idx[idx]]]\n",
    "            \n",
    "        if roi and len(roi) == 4:\n",
    "            size_y, size_x = double_frame[0].shape\n",
    "            min_x, max_x = max(0, roi[0]-1), min(roi[1], size_x)\n",
    "            min_y, max_y = max(0, roi[2]-1), min(roi[3], size_x)\n",
    "            \n",
    "            double_frame[0] = np.array(double_frame[0][min_y:max_y, min_x:max_x])\n",
    "            double_frame[1] = np.array(double_frame[1][min_y:max_y, min_x:max_x])\n",
    "\n",
    "        images_double_framed += [double_frame]\n",
    "            \n",
    "    return np.array(images_double_framed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67afe591",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Externals\n",
    "\n",
    "import math\n",
    "import numpy.matlib as npmb\n",
    "\n",
    "\n",
    "## Prepare images for PIV\n",
    "# Determine which indices must be used to create the interrogation windows. \n",
    "# It also add a padding dark color to the images.\n",
    "#\n",
    "# Output: Indexes for vectors (MinX, MaxX, MinY, MaxY), the padded images and the interrogation window indexes.\n",
    "\n",
    "def prepare_piv_images(images, window_size, step):\n",
    "    \n",
    "    # Calculating vectors.\n",
    "    min_x = 1 + math.ceil(step)\n",
    "    min_y = 1 + math.ceil(step)\n",
    "    size_y, size_x = first(images)[0].shape\n",
    "    max_x = step * math.floor(size_x / step) - (window_size - 1) + math.ceil(step)\n",
    "    max_y = step * math.floor(size_y / step) - (window_size - 1) + math.ceil(step)\n",
    "    vectors_u = math.floor((max_x - min_x)/step + 1)\n",
    "    vectors_v = math.floor((max_y - min_y)/step + 1)\n",
    "    \n",
    "    # Centering image grid.\n",
    "    pad_x = size_x - max_x\n",
    "    pad_y = size_y - max_y\n",
    "    shift_x = max(0, round((pad_x - min_x) / 2))\n",
    "    shift_y = max(0, round((pad_y - min_y) / 2))\n",
    "    min_x += shift_x\n",
    "    min_y += shift_y\n",
    "    max_x += shift_x\n",
    "    max_y += shift_y\n",
    "    \n",
    "    # Adding a dark padded border to images.\n",
    "    padded_images = []\n",
    "    for idx in range(len(images)):\n",
    "        padded_images += [[]]\n",
    "        for frame in range(2):\n",
    "            image = images[idx][frame]\n",
    "            padded_images[idx] += [np.pad(image, math.ceil(window_size-step), constant_values=image.min())]\n",
    "        padded_images[idx] = np.array(padded_images[idx])\n",
    "    padded_images = np.array(padded_images)\n",
    "    \n",
    "    # Interrogation window indexes for first frame.\n",
    "    padded_size_y, padded_size_x = first(padded_images)[0].shape\n",
    "    min_s0 = npmb.repmat(np.array(np.arange(min_y, max_y + 1, step) - 1)[:, None], 1, vectors_u)\n",
    "    max_s0 = npmb.repmat(np.array(np.arange(min_x, max_x + 1, step) - 1) * padded_size_y, vectors_v, 1)\n",
    "    s0 = np.asarray(min_s0 + max_s0).flatten()[..., np.newaxis, np.newaxis].transpose([1, 2, 0])\n",
    "\n",
    "    min_s1 = npmb.repmat(np.array(np.arange(1, window_size + 1))[:, None], 1, window_size)\n",
    "    max_s1 = npmb.repmat(np.array(np.arange(1, window_size + 1) - 1) * padded_size_y, window_size, 1)\n",
    "    s1 = min_s1 + max_s1\n",
    "\n",
    "    indexes = np.tile(np.asarray(s1)[..., np.newaxis], [1, 1, s0.shape[2]]) + np.tile(s0, [window_size, window_size, 1]) - 1\n",
    "    \n",
    "    return min_x, max_x, min_y, max_y, padded_images, indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a90487a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Externals\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "## Cumulative cross correlation\n",
    "# Averages correlation maps from an image stack.\n",
    "#\n",
    "# TODO: This function isn't working properly! Matlab FFT ≠ Numpy FFT.\n",
    "# Should fix the cross correlation calculation and also check the normalization (different shape expected).\n",
    "#\n",
    "# Output: A correlation matrix with the same size as the images input.\n",
    "\n",
    "NORMALIZED_CORRELATION_RESOLUTION = 2**8\n",
    "def cumulative_cross_correlation(images, indexes, window_size):\n",
    "    \n",
    "    total_correlation = 0\n",
    "    for idx, image in enumerate(images):\n",
    "        frame_a = image[0].take(indexes).astype(np.single)\n",
    "        frame_b = image[1].take(indexes).astype(np.single)\n",
    "        \n",
    "        # Calculating cross correlation\n",
    "        fft_a = np.fft.fft2(frame_a)\n",
    "        fft_b = np.fft.fft2(frame_b)\n",
    "\n",
    "        fft_shifting = np.real(np.fft.ifft(np.fft.ifft(np.conj(fft_a) * fft_b, window_size, 1), window_size, 0))\n",
    "        correlation = np.fft.fftshift(np.fft.fftshift(fft_shifting, 2), 1)\n",
    "        correlation[correlation < 0] = 0\n",
    "        \n",
    "        # Normalizing correlation\n",
    "        min_corr = np.tile(correlation.min(0).min(0), [correlation.shape[0], correlation.shape[1], 1])\n",
    "        max_corr = np.tile(correlation.max(0).max(0), [correlation.shape[0], correlation.shape[1], 1])\n",
    "        norm_corr = (correlation - min_corr) / (max_corr - min_corr) * (NORMALIZED_CORRELATION_RESOLUTION - 1)\n",
    "    \n",
    "        total_correlation += norm_corr/len(images)\n",
    "        \n",
    "    return total_correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3cec7b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Externals\n",
    "\n",
    "import numpy as np\n",
    "import scipy.ndimage\n",
    "\n",
    "\n",
    "## Vector field determination\n",
    "# Here it's where magic happens, calculating peaks and doing science stuff to get the proper PIV data.\n",
    "#\n",
    "# Output: OutputPIV object\n",
    "\n",
    "S2N_FILTER = False\n",
    "DEFAULT_S2N_THRESHOLD = 1\n",
    "DEFAULT_RES_NORMALIZATION = 255\n",
    "def vector_field_determination(correlation, int_window, step, min_x, max_x, min_y, max_y):\n",
    "    \n",
    "    # Normalize result\n",
    "    squeezed_min_corr = correlation.min(0).min(0).squeeze()[:, np.newaxis, np.newaxis]\n",
    "    squeezed_delta_corr = correlation.max(0).max(0).squeeze()[:, np.newaxis, np.newaxis] - squeezed_min_corr\n",
    "    min_res = np.tile(squeezed_min_corr, [1, correlation.shape[0], correlation.shape[1]]).transpose([1, 2, 0])\n",
    "    delta_res = np.tile(squeezed_delta_corr, [1, correlation.shape[0], correlation.shape[1]]).transpose([1, 2, 0])\n",
    "    corr = ((correlation - min_res) / delta_res) * DEFAULT_RES_NORMALIZATION\n",
    "    \n",
    "    # Find peaks and S2N\n",
    "    x1, y1, indexes1, x2, y2, indexes2, s2n = find_all_displacements(corr)\n",
    "    \n",
    "    # Sub-pixel determination\n",
    "    pixel_offset = 1 if (int_window % 2 == 0) else 0.5\n",
    "    vector = sub_pixel_gaussian(corr, int_window, x1, y1, indexes1, pixel_offset)\n",
    "    \n",
    "    # Create data\n",
    "    x_range = np.arange(min_x, max_x + 1, step)\n",
    "    y_range = np.arange(min_y, max_y + 1, step)\n",
    "    output_x = np.tile(x_range + int_window / 2, [len(y_range), 1])\n",
    "    output_y = np.tile(y_range[:, None] + int_window / 2, [1, len(x_range)])\n",
    "    vector = np.reshape(vector, np.append(np.array(output_x.transpose().shape), 2), order='F').transpose([1, 0, 2])\n",
    "\n",
    "    # Signal to noise filter\n",
    "    s2n = s2n[np.reshape(np.array(range(output_x.size)), output_x.transpose().shape, order='F').transpose()]\n",
    "    if S2N_FILTER:\n",
    "        vector[:,:,0] = vector[:,:,0] * (s2n > DEFAULT_S2N_THRESHOLD)\n",
    "        vector[:,:,1] = vector[:,:,1] * (s2n > DEFAULT_S2N_THRESHOLD)\n",
    "    \n",
    "    output_u = vector[:,:,0]\n",
    "    output_v = vector[:,:,1]\n",
    "\n",
    "    output_x -= int_window/2\n",
    "    output_y -= int_window/2\n",
    "\n",
    "    return OutputPIV(output_x, output_y, output_u, output_v, s2n)\n",
    "    \n",
    "    \n",
    "## Gaussian sub-pixel mode\n",
    "# No f*cking clue what this does. Crazy math shit.\n",
    "#\n",
    "# Output: A vector with a sub-pixel deviation - Maybe? I'm not sure. Its dimensions are Number-of-Correlations by 2. \n",
    "\n",
    "def sub_pixel_gaussian(correlation, int_window, x, y, indexes, pixel_offset):\n",
    "    z = np.array(range(indexes.shape[0])).transpose()\n",
    "    \n",
    "    xi = np.nonzero(np.logical_not(np.logical_and(\n",
    "        # Adjusting -1 to -2 according to Matlab/Python mapping.\n",
    "        np.logical_and(x <= correlation.shape[1] - 2, y <= correlation.shape[0] - 2),\n",
    "        np.logical_and(x >= 2, y >= 2)\n",
    "    )))[0]\n",
    "\n",
    "    x = np.delete(x, xi)\n",
    "    y = np.delete(y, xi)\n",
    "    z = np.delete(z, xi)\n",
    "    x_max = correlation.shape[1]\n",
    "    vector = np.ones((correlation.shape[2], 2)) * np.nan\n",
    "\n",
    "    if len(x) > 0:\n",
    "        ip = np.ravel_multi_index(np.array([x, y, z]), correlation.shape, order='F')\n",
    "        flattened_correlation = correlation.flatten(order='F')\n",
    "\n",
    "        f0 = np.log(flattened_correlation[ip])\n",
    "        f1 = np.log(flattened_correlation[ip - 1])\n",
    "        f2 = np.log(flattened_correlation[ip + 1])\n",
    "        peak_y = y + (f1 - f2) / (2 * f1 - 4 * f0 + 2 * f2)\n",
    "\n",
    "        f1 = np.log(flattened_correlation[ip - x_max])\n",
    "        f2 = np.log(flattened_correlation[ip + x_max])\n",
    "        peak_x = y + (f1 - f2) / (2 * f1 - 4 * f0 + 2 * f2)\n",
    "    \n",
    "        sub_pixel_x = peak_x - (int_window / 2) - pixel_offset\n",
    "        sub_pixel_y = peak_y - (int_window / 2) - pixel_offset\n",
    "    \n",
    "        vector[z, :] = np.array([sub_pixel_x, sub_pixel_y]).transpose()\n",
    "    \n",
    "    return vector\n",
    "\n",
    "    \n",
    "## Find all displacements\n",
    "# Find all integer pixel displacement in a stack of correlation windows.\n",
    "#\n",
    "# Output: Horizontal and vertical indexes of the first and second maximum for each slice of correlation in the third\n",
    "# dimension (PeakX1, PeackY1, PeakX2, PeakY2), the absolute indexes of the correlation maximums (Idx1, Idx2) and the\n",
    "# ratio between the first and second peack (S2N) - 0 indicates non confiable results.\n",
    "\n",
    "def find_all_displacements(correlation):\n",
    "    corr_size = correlation.shape[0]\n",
    "    \n",
    "    # Finding first peak\n",
    "    peak1_val, peak1_x, peak1_y, peak_indexes1, peak_positions1 = find_peaks(correlation)\n",
    "\n",
    "    # Finding second peak (1 extra point from Matlab size)\n",
    "    filter_size = 10 if corr_size >= 64 else 5 if corr_size >= 32 else 4\n",
    "    filtered = scipy.ndimage.correlate(peak_positions1, np.ones([filter_size, filter_size, 1]), mode='constant')\n",
    "    correlation = (1 - filtered) * correlation\n",
    "    peak2_val, peak2_x, peak2_y, peak_indexes2, _ = find_peaks(correlation)\n",
    "\n",
    "    # Calculating Signal to Noise ratio\n",
    "    signal_to_noise = np.zeros([peak1_val.shape[0]])\n",
    "    signal_to_noise[peak2_val != 0] = peak1_val[peak2_val != 0] / peak2_val[peak2_val != 0]\n",
    "\n",
    "    # Maximum at a border usually indicates that MAX took the first one it found, so we should put a bad S2N, like 0.\n",
    "    signal_to_noise[peak1_y == 0] = 0\n",
    "    signal_to_noise[peak1_x == 0] = 0\n",
    "    signal_to_noise[peak1_y == (corr_size - 1)] = 0\n",
    "    signal_to_noise[peak1_x == (corr_size - 1)] = 0\n",
    "    signal_to_noise[peak2_y == 0] = 0\n",
    "    signal_to_noise[peak2_x == 0] = 0\n",
    "    signal_to_noise[peak2_y == (corr_size - 1)] = 0\n",
    "    signal_to_noise[peak2_x == (corr_size - 1)] = 0\n",
    "    \n",
    "    return peak1_x, peak1_y, peak_indexes2, peak2_x, peak2_y, peak_indexes2, signal_to_noise\n",
    "    \n",
    "    \n",
    "## Find peaks\n",
    "# Find max values for each correlation.\n",
    "#\n",
    "# Output: The MAX peak, its coordinates (X and Y) and the indexes.\n",
    "    \n",
    "def find_peaks(correlation):\n",
    "    corr_size = correlation.shape[0]\n",
    "    corr_numbers = correlation.shape[2]\n",
    "    max_peak = correlation.max(0).max(0)\n",
    "    max_positions = correlation == np.tile(max_peak[np.newaxis, np.newaxis, ...], [corr_size, corr_size, 1])\n",
    "    max_indexes = np.where(max_positions.transpose(2, 1, 0).flatten())[0]\n",
    "    peak_y, peak_x, peak_z = np.unravel_index(max_indexes, (corr_size, corr_size, corr_numbers), order='F')\n",
    "\n",
    "    # If two elements equals to the max we should check if they are in the same layer and take the first one.\n",
    "    # Surely the second one will be the second highest peak. Anyway this would be a bad vector.\n",
    "    unique_max_indexes = np.unique(peak_z)\n",
    "    max_indexes = max_indexes[unique_max_indexes]\n",
    "    peak_x = peak_x[unique_max_indexes]\n",
    "    peak_y = peak_y[unique_max_indexes]\n",
    "    \n",
    "    return max_peak, peak_x, peak_y, max_indexes, max_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "51a3c516",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Externals\n",
    "\n",
    "import numpy as np\n",
    "import scipy.sparse\n",
    "import scipy.ndimage\n",
    "import scipy.sparse.linalg\n",
    "\n",
    "\n",
    "# Filter fields (WIP)\n",
    "# Applies different filters on the vector fields.\n",
    "#\n",
    "# Output: OutputPIV object, with filtered data.\n",
    "\n",
    "B = 1\n",
    "EPSILON = 0.02\n",
    "DEFAULT_THRESH = 1.5\n",
    "DEFAULT_STD_THRESHOLD = 4\n",
    "def filter_fields(data, std_threashold=DEFAULT_STD_THRESHOLD):\n",
    "    # Filter 1: Threshold on signal to noise.\n",
    "    data.u = remove_nans(data.u)\n",
    "    data.v = remove_nans(data.v)\n",
    "\n",
    "    # Filter 2:\n",
    "    mean_u = np.mean(data.u)\n",
    "    mean_v = np.mean(data.v)\n",
    "    std_u = np.std(data.u, ddof=1)\n",
    "    std_v = np.std(data.v, ddof=1)\n",
    "    min_u = mean_u - std_threashold * std_u\n",
    "    max_u = mean_u + std_threashold * std_u\n",
    "    min_v = mean_v - std_threashold * std_v\n",
    "    max_v = mean_v + std_threashold * std_u\n",
    "    data.u[data.u < min_u] = np.nan\n",
    "    data.u[data.u > max_u] = np.nan\n",
    "    data.v[data.v < min_v] = np.nan\n",
    "    data.v[data.v > max_v] = np.nan\n",
    "\n",
    "    # Filter 3:\n",
    "    size_y, size_x = data.u.shape\n",
    "    normal_fluctuation = np.zeros(shape=(size_y, size_x, 2))\n",
    "\n",
    "    for it in range(2):\n",
    "        velocity_comparator = data.u if it == 0 else data.v\n",
    "        neighbors = np.empty(shape=(size_y - 2, size_x - 2, 2 * B + 1, 2 * B + 1))\n",
    "\n",
    "        for ii in range(-B, B + 1):\n",
    "            for jj in range(-B, B + 1):\n",
    "                ii_start = 1 + B - 1 + ii\n",
    "                ii_end = -B + ii if -B + ii < 0 else None\n",
    "                jj_start = 1 + B - 1 + jj\n",
    "                jj_end = -B + jj if -B + jj < 0 else None\n",
    "\n",
    "                ii_neighbors = ii + 2 * B - 1\n",
    "                jj_neighbors = jj + 2 * B - 1\n",
    "\n",
    "                neighbors[:, :, ii_neighbors, jj_neighbors] = velocity_comparator[ii_start:ii_end, jj_start:jj_end]\n",
    "\n",
    "        first_neighbors = np.arange((2 * B + 1) * B + B)\n",
    "        last_neighbors = np.arange((2 * B + 1) * B + B + 1, (2 * B + 1) ** 2)\n",
    "        neighbors_column = np.reshape(neighbors, [neighbors.shape[0], neighbors.shape[1], (2 * B + 1) ** 2], order='F')\n",
    "        neighbors_column2 = neighbors_column[:, :, np.append(first_neighbors, last_neighbors)].transpose([2, 0, 1])\n",
    "\n",
    "        median = np.median(neighbors_column2, axis=0).transpose()\n",
    "        velocity_comparator2 = velocity_comparator[B:-B, B:-B]\n",
    "        fluctuation = velocity_comparator2 - median.transpose()\n",
    "        result = neighbors_column2 - np.tile(median, [(2 * B + 1) ** 2 - 1, 1, 1]).transpose([0, 2, 1])\n",
    "\n",
    "        median_result = np.median(np.abs(result), axis=0)\n",
    "        normal_fluctuation[B:-B, B:-B, it] = np.abs(fluctuation / (median_result + EPSILON))\n",
    "\n",
    "    info = np.sqrt(normal_fluctuation[:, :, 0] ** 2 + normal_fluctuation[:, :, 1] ** 2) > DEFAULT_THRESH\n",
    "    data.u[info] = np.nan\n",
    "    data.v[info] = np.nan\n",
    "\n",
    "    # Inpaint NANs\n",
    "    data.u = inpaint_nans(data.u)\n",
    "    data.v = inpaint_nans(data.v)\n",
    "\n",
    "    # Filter 4:\n",
    "    try:\n",
    "\n",
    "        # Trying to apply the smooth predictor.\n",
    "        data.u = smooth(data.u)\n",
    "        data.v = smooth(data.v)\n",
    "\n",
    "    except:\n",
    "\n",
    "        # Applying Gaussian filter instead.\n",
    "        gfilter = gaussian_filter(5, 1)\n",
    "        data.u = scipy.ndimage.convolve(data.u, gfilter, mode='nearest')\n",
    "        data.v = scipy.ndimage.convolve(data.v, gfilter, mode='nearest')\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "# Remove NANs\n",
    "# Replace all the NANs from a data vector with a custom interpolation calculated with its values.\n",
    "#\n",
    "# Output: A matrix with the same dimensions ang items as the input, but with NANs replaced.\n",
    "\n",
    "DEFAULT_PATCH_SIZE = 1\n",
    "def remove_nans(data, patch_size=DEFAULT_PATCH_SIZE):\n",
    "    both_nan_indexes = list(zip(*np.where(np.isnan(data))))\n",
    "    size_y, size_x = data.shape\n",
    "\n",
    "    fixed_data = data.copy()\n",
    "    for y_idx, x_idx in both_nan_indexes:\n",
    "        sample = data[\n",
    "            max(0, y_idx - patch_size):min(size_y, y_idx + patch_size + 1),\n",
    "            max(0, x_idx - patch_size):min(size_x, x_idx + patch_size + 1)\n",
    "        ]\n",
    "\n",
    "        sample = sample[~np.isnan(sample)]\n",
    "        new_data = np.median(sample) if sample.size > 0 else 0\n",
    "\n",
    "        fixed_data[y_idx, x_idx] = new_data\n",
    "\n",
    "    return fixed_data\n",
    "\n",
    "\n",
    "# Inpaint NANs\n",
    "# Solves approximation to one of several pdes to interpolate and extrapolate holes in an array.\n",
    "# It uses a spring metaphor, assuming they (with a nominal length of zero) connect each node with every neighbor\n",
    "# (horizontally, vertically and diagonally). Since each node tries to be like its neighbors, extrapolation is as a\n",
    "# constant function where this is consistent with the neighboring nodes.\n",
    "#\n",
    "# Output: A matrix with the same dimensions ang items as the input, but with NANs replaced.\n",
    "\n",
    "DEFAULT_SPRING_ITERATIONS = 4\n",
    "def inpaint_nans(data, iterations=DEFAULT_SPRING_ITERATIONS):\n",
    "    size_y, size_x = data.shape\n",
    "    flattened = data.flatten(order='F')\n",
    "\n",
    "    # List the nodes which are known, and which will be interpolated.\n",
    "    nan_indexes = np.where(np.isnan(flattened))[0]\n",
    "    known_indexes = np.where(~np.isnan(flattened))[0]\n",
    "\n",
    "    # Get total NANs overall.\n",
    "    nan_count = nan_indexes.size\n",
    "\n",
    "    # Convert NAN indexes to [Row, Column] form.\n",
    "    indexes_y, indexes_x = np.unravel_index(nan_indexes, (size_y, size_x), order='F')\n",
    "\n",
    "    # All forms of index in one array: 0 - Unrolled ; 1 - Row ; 2 - Column\n",
    "    nan_list = np.array([nan_indexes, indexes_y, indexes_x]).transpose() + 1\n",
    "\n",
    "    # Spring analogy - interpolating operator.\n",
    "    # List of all springs between a node and a horizontal or vertical neighbor.\n",
    "    hv_list = np.array([[-1, -1, 0], [1, 1, 0], [-size_y, 0, -1], [size_y, 0, 1]])\n",
    "    hv_springs = np.empty((0, 2))\n",
    "\n",
    "    for it in range(iterations):\n",
    "        hvs = nan_list + np.tile(hv_list[it, :], (nan_count, 1))\n",
    "        k = np.logical_and(\n",
    "            np.logical_and(hvs[:, 1] >= 1, hvs[:, 1] <= size_y),\n",
    "            np.logical_and(hvs[:, 2] >= 1, hvs[:, 2] <= size_x)\n",
    "        )\n",
    "        hv_springs = np.append(hv_springs, np.array([nan_list[k, 0], hvs[k, 0]]).transpose(), axis=0)\n",
    "\n",
    "    # Delete replicate springs\n",
    "    hv_springs.sort(axis=1)\n",
    "    hv_springs = np.unique(hv_springs, axis=0) - 1\n",
    "\n",
    "    # Build sparse matrix of connections.\n",
    "    # Springs connecting diagonal neighbors are weaker than the horizontal and vertical ones.\n",
    "    nhv = hv_springs.shape[0]\n",
    "    I, V = np.tile(np.arange(0, nhv)[:, None], (1, 2)).flatten(), np.tile([1, -1], (nhv, 1)).flatten()\n",
    "    springs = scipy.sparse.csr_matrix((V, (I, hv_springs.flatten())), shape=(nhv, data.size))\n",
    "    springs.eliminate_zeros()\n",
    "\n",
    "    # Eliminate knowns\n",
    "    rhs = springs[:, known_indexes] * flattened[known_indexes] * -1\n",
    "\n",
    "    # Solve problem\n",
    "    output = flattened\n",
    "    solution, _, _, _, _, _, _, _, _, _ = scipy.sparse.linalg.lsqr(springs[:, nan_indexes], rhs)\n",
    "    output[nan_indexes] = solution\n",
    "\n",
    "    return np.reshape(output, (size_x, size_y)).transpose()\n",
    "\n",
    "\n",
    "# Smooth predictor\n",
    "# Fast, automatized and robust discrete spline smoothing for data of arbitrary dimension.\n",
    "# Automatically smooths the uniformly-sampled input array. It can be any N-D noisy array (time series, images,\n",
    "# 3D data, ...). Non finite data (NaN or Inf) are treated as missing values.\n",
    "#\n",
    "# Output: A matrix with the same dimensions ang items as the input, but with NANs replaced.\n",
    "\n",
    "def smooth(data):\n",
    "    return octave_cli.smoothn(data)\n",
    "\n",
    "\n",
    "# Gaussian filter\n",
    "# Returns a Gaussian filter with the same implementation as Matlab.\n",
    "#\n",
    "# Output: A matrix that works as a Gaussian filter.\n",
    "\n",
    "def gaussian_filter(size=3, sigma=0.5):\n",
    "    m, n = [(ss-1.)/2. for ss in (size, size)]\n",
    "    y, x = np.ogrid[-m:m+1, -n:n+1]\n",
    "    h = np.exp(-(x*x + y*y) / (2.*sigma*sigma))\n",
    "    h[h < np.finfo(h.dtype).eps * h.max()] = 0\n",
    "    sumh = h.sum()\n",
    "    if sumh != 0:\n",
    "        h /= sumh\n",
    "    return h\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fead6257",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculate PIV\n",
    "# Generate the PIV data from the images loaded with the input parameters.\n",
    "#\n",
    "# Output: OutputPIV object\n",
    "\n",
    "DEFAULT_OVERLAP = 0.5\n",
    "def PIV(images, int_window, overlap=DEFAULT_OVERLAP):\n",
    "    step = round(int_window * overlap)\n",
    "    min_x, max_x, min_y, max_y, padded_images, indexes = prepare_piv_images(images, int_window, step)\n",
    "    cross_correlation = cumulative_cross_correlation(padded_images, indexes, int_window)\n",
    "    raw_piv_data = vector_field_determination(cross_correlation, int_window, step, min_x, max_x, min_y, max_y)\n",
    "    filtered_piv_data = filter_fields(raw_piv_data)\n",
    "\n",
    "    filtered_piv_data.x = filtered_piv_data.x.transpose()\n",
    "    filtered_piv_data.y = filtered_piv_data.y.transpose()\n",
    "    filtered_piv_data.u = filtered_piv_data.u.transpose()\n",
    "    filtered_piv_data.v = filtered_piv_data.v.transpose()\n",
    "    filtered_piv_data.s2n = filtered_piv_data.s2n.transpose()\n",
    "    \n",
    "    return filtered_piv_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "268dd64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Externals\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "## Communication Exceptions\n",
    "# Exception thrown when some parameters weren't passed as expected.\n",
    "        \n",
    "class InvalidParametersError(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "## Prepare output\n",
    "# Get the velocity for the desired point. If it is not possible, it will get it for the closest point.\n",
    "#\n",
    "# Output: OutputPIV object\n",
    "\n",
    "def prepare_output(center_x, center_y, piv_data):\n",
    "    idx_x = (np.abs(piv_data.x[:,1] - center_x)).argmin()\n",
    "    idx_y = (np.abs(piv_data.y[1,:] - center_y)).argmin()\n",
    "\n",
    "    position_x = int(piv_data.x[idx_x,1]) + 1\n",
    "    position_y = int(piv_data.y[1,idx_y]) + 1\n",
    "    velocity_x = piv_data.u[idx_x,idx_y]\n",
    "    velocity_y = piv_data.v[idx_x,idx_y]\n",
    "    signal_to_noise = piv_data.s2n[idx_x,idx_y]\n",
    "    \n",
    "    return OutputPIV(position_x, position_y, velocity_x, velocity_y, signal_to_noise)\n",
    "\n",
    "\n",
    "## Entrypoint\n",
    "# Retrieve the images, prepare them and calculate the PIV computation.\n",
    "#\n",
    "# Output: OutputPIV object\n",
    "\n",
    "DEFAULT_INTERROGATION_WINDOW = 32\n",
    "def calculate_piv(frontend_data):\n",
    "    results = {}\n",
    "    settings = frontend_data.settings\n",
    "    \n",
    "    # TODO: Check if this could be parallelized to increase performance.\n",
    "    for point_id, point_data in frontend_data.points.items():\n",
    "\n",
    "        double_framed_images = single_to_double_frame(point_data.images)\n",
    "        if double_framed_images.size <= 2:\n",
    "            raise InvalidParametersError(f'Not enough images passed for point {point_id}')\n",
    "            \n",
    "        shift_x = 0\n",
    "        shift_y = 0\n",
    "        if settings.roi_size is not None:\n",
    "            roi_shift = int(settings.roi_size / 2)\n",
    "            shift_x = point_data.pos_x - roi_shift\n",
    "            shift_y = point_data.pos_y - roi_shift\n",
    "        \n",
    "        piv_data = PIV(double_framed_images, settings.window_size)\n",
    "        piv_data.x = piv_data.x * settings.scale + shift_x\n",
    "        piv_data.y = piv_data.y * settings.scale + shift_y\n",
    "        piv_data.u = piv_data.u * settings.scale / settings.time_delta\n",
    "        piv_data.v = piv_data.v * settings.scale / settings.time_delta\n",
    "        \n",
    "        point_results = prepare_output(point_data.pos_x - 1, point_data.pos_y - 1, piv_data)\n",
    "        results[point_id] = point_results\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "df83655f",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = load_images()\n",
    "window_size, overlap = 32, 0.5\n",
    "\n",
    "step = round(window_size * overlap)\n",
    "double_framed = single_to_double_frame(inputs)\n",
    "min_x, max_x, min_y, max_y, images, indexes = prepare_piv_images(double_framed, window_size, step)\n",
    "\n",
    "## CUMULATIVE CROSS CORRELATION\n",
    "\n",
    "NORMALIZED_CORRELATION_RESOLUTION = 2**8\n",
    "    \n",
    "total_correlation = 0\n",
    "for idx, image in enumerate(images):\n",
    "    frame_a = image[0].take(indexes).astype(np.single)\n",
    "    frame_b = image[1].take(indexes).astype(np.single)\n",
    "        \n",
    "    # Calculating cross correlation\n",
    "    fft_a = np.fft.fft2(frame_a)\n",
    "    fft_b = np.fft.fft2(frame_b)\n",
    "\n",
    "    fft_shifting = np.real(np.fft.ifft(np.fft.ifft(np.conj(fft_a) * fft_b, window_size, 1), window_size, 0))\n",
    "    correlation = np.fft.fftshift(np.fft.fftshift(fft_shifting, 2), 1)\n",
    "    correlation[correlation < 0] = 0\n",
    "        \n",
    "    # Normalizing correlation\n",
    "    min_corr = np.tile(correlation.min(0).min(0), [correlation.shape[0], correlation.shape[1], 1])\n",
    "    max_corr = np.tile(correlation.max(0).max(0), [correlation.shape[0], correlation.shape[1], 1])\n",
    "    norm_corr = (correlation - min_corr) / (max_corr - min_corr) * (NORMALIZED_CORRELATION_RESOLUTION - 1)\n",
    "    \n",
    "    total_correlation += norm_corr/len(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "79d10ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = load_images()\n",
    "window_size, overlap = 32, 0.5\n",
    "\n",
    "step = round(window_size * overlap)\n",
    "double_framed = single_to_double_frame(inputs)\n",
    "min_x, max_x, min_y, max_y, images, indexes = prepare_piv_images(double_framed, window_size, step)\n",
    "\n",
    "## CUMULATIVE CROSS CORRELATION\n",
    "    \n",
    "total_correlation = 0\n",
    "for idx, image in enumerate(images):\n",
    "    frame_a = image[0].flatten(order='F').take(indexes).astype(np.single)\n",
    "    frame_b = image[1].flatten(order='F').take(indexes).astype(np.single)\n",
    "        \n",
    "    # Calculating cross correlation\n",
    "    correlation = octave_cli.correlate(frame_a, frame_b, window_size)\n",
    "        \n",
    "    # Normalizing correlation\n",
    "    min_corr = np.tile(correlation.min(0).min(0), [correlation.shape[0], correlation.shape[1], 1])\n",
    "    max_corr = np.tile(correlation.max(0).max(0), [correlation.shape[0], correlation.shape[1], 1])\n",
    "    norm_corr = (correlation - min_corr) / (max_corr - min_corr) * (NORMALIZED_CORRELATION_RESOLUTION - 1)\n",
    "    \n",
    "    total_correlation += norm_corr/len(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2e4a51d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexes_fix(indexes):\n",
    "    if indexes[:,0].min() > 0:\n",
    "        indexes[:,0] -= 1\n",
    "    if indexes[:,1].min() > 0:\n",
    "        indexes[:,1] -= 1\n",
    "    return indexes\n",
    "\n",
    "def fix_filter(full_filter):\n",
    "    indexes_y, indexes_x, indexes_z = np.where(full_filter)\n",
    "    indexes = np.array(list(zip(indexes_y, indexes_x, indexes_z)))\n",
    "    indexes = np.sort(indexes.view('i8,i8,i8'), order=['f2'], axis=0).view('i8')\n",
    "    grouped = np.split(indexes[:,:], np.unique(indexes[:,2], return_index=True)[1])[1:]\n",
    "\n",
    "    new_indexes = np.array(list(map(indexes_fix, grouped)))\n",
    "    new_indexes = np.reshape(new_indexes, (new_indexes.shape[0] * new_indexes.shape[1], 3))\n",
    "\n",
    "    new_indexes_y, new_indexes_x, new_indexes_z = np.split(new_indexes, 3, axis=1)\n",
    "    new_full_filter = np.zeros(full_filter.shape)\n",
    "    new_full_filter[new_indexes_y, new_indexes_x, new_indexes_z] = True\n",
    "    \n",
    "    return new_full_filter\n",
    "\n",
    "def fix_filter2(full_filter):\n",
    "    indexes_y, indexes_x, indexes_z = np.where(full_filter)\n",
    "    indexes = np.array(list(zip(indexes_y, indexes_x, indexes_z)))\n",
    "    grouped = np.array(group_by(indexes, lambda index: index[2]))\n",
    "\n",
    "    new_indexes = np.array(list(map(indexes_fix, grouped)))\n",
    "    new_indexes = np.reshape(new_indexes, (new_indexes.shape[0] * new_indexes.shape[1], 3))\n",
    "\n",
    "    new_indexes_y, new_indexes_x, new_indexes_z = np.split(new_indexes, 3, axis=1)\n",
    "    new_full_filter = np.zeros(full_filter.shape)\n",
    "    new_full_filter[new_indexes_y, new_indexes_x, new_indexes_z] = True\n",
    "    \n",
    "    return new_full_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "03ee589c",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_filter = np.zeros((10, 10, 5))\n",
    "full_filter[3,5,0] = 1\n",
    "full_filter[3,6,0] = 1\n",
    "full_filter[3,7,0] = 1\n",
    "full_filter[3,8,0] = 1\n",
    "full_filter[4,5,0] = 1\n",
    "full_filter[4,6,0] = 1\n",
    "full_filter[4,7,0] = 1\n",
    "full_filter[4,8,0] = 1\n",
    "full_filter[5,5,0] = 1\n",
    "full_filter[5,6,0] = 1\n",
    "full_filter[5,7,0] = 1\n",
    "full_filter[5,8,0] = 1\n",
    "full_filter[6,5,0] = 1\n",
    "full_filter[6,6,0] = 1\n",
    "full_filter[6,7,0] = 1\n",
    "full_filter[6,8,0] = 1\n",
    "full_filter[2,5,1] = 1\n",
    "full_filter[2,6,1] = 1\n",
    "full_filter[2,7,1] = 1\n",
    "full_filter[2,8,1] = 1\n",
    "full_filter[3,5,1] = 1\n",
    "full_filter[3,6,1] = 1\n",
    "full_filter[3,7,1] = 1\n",
    "full_filter[3,8,1] = 1\n",
    "full_filter[4,5,1] = 1\n",
    "full_filter[4,6,1] = 1\n",
    "full_filter[4,7,1] = 1\n",
    "full_filter[4,8,1] = 1\n",
    "full_filter[5,5,1] = 1\n",
    "full_filter[5,6,1] = 1\n",
    "full_filter[5,7,1] = 1\n",
    "full_filter[5,8,1] = 1\n",
    "full_filter[0,5,2] = 1\n",
    "full_filter[0,6,2] = 1\n",
    "full_filter[0,7,2] = 1\n",
    "full_filter[0,8,2] = 1\n",
    "full_filter[1,5,2] = 1\n",
    "full_filter[1,6,2] = 1\n",
    "full_filter[1,7,2] = 1\n",
    "full_filter[1,8,2] = 1\n",
    "full_filter[2,5,2] = 1\n",
    "full_filter[2,6,2] = 1\n",
    "full_filter[2,7,2] = 1\n",
    "full_filter[2,8,2] = 1\n",
    "full_filter[3,5,2] = 1\n",
    "full_filter[3,6,2] = 1\n",
    "full_filter[3,7,2] = 1\n",
    "full_filter[3,8,2] = 1\n",
    "full_filter[0,0,3] = 1\n",
    "full_filter[0,1,3] = 1\n",
    "full_filter[0,2,3] = 1\n",
    "full_filter[0,3,3] = 1\n",
    "full_filter[1,0,3] = 1\n",
    "full_filter[1,1,3] = 1\n",
    "full_filter[1,2,3] = 1\n",
    "full_filter[1,3,3] = 1\n",
    "full_filter[2,0,3] = 1\n",
    "full_filter[2,1,3] = 1\n",
    "full_filter[2,2,3] = 1\n",
    "full_filter[2,3,3] = 1\n",
    "full_filter[3,0,3] = 1\n",
    "full_filter[3,1,3] = 1\n",
    "full_filter[3,2,3] = 1\n",
    "full_filter[3,3,3] = 1\n",
    "full_filter[2,0,4] = 1\n",
    "full_filter[3,0,4] = 1\n",
    "full_filter[4,0,4] = 1\n",
    "full_filter[5,0,4] = 1\n",
    "full_filter[2,1,4] = 1\n",
    "full_filter[3,1,4] = 1\n",
    "full_filter[4,1,4] = 1\n",
    "full_filter[5,1,4] = 1\n",
    "full_filter[2,2,4] = 1\n",
    "full_filter[3,2,4] = 1\n",
    "full_filter[4,2,4] = 1\n",
    "full_filter[5,2,4] = 1\n",
    "full_filter[2,3,4] = 1\n",
    "full_filter[3,3,4] = 1\n",
    "full_filter[4,3,4] = 1\n",
    "full_filter[5,3,4] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eab56ddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filter[:,:,0] = \n",
      "\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 1. 1. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 1. 1. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 1. 1. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 1. 1. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      "----------------------\n",
      "filter[:,:,1] = \n",
      "\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 1. 1. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 1. 1. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 1. 1. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 1. 1. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      "----------------------\n",
      "filter[:,:,2] = \n",
      "\n",
      "[[0. 0. 0. 0. 0. 1. 1. 1. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 1. 1. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 1. 1. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 1. 1. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      "----------------------\n",
      "filter[:,:,3] = \n",
      "\n",
      "[[1. 1. 1. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      "----------------------\n",
      "filter[:,:,4] = \n",
      "\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      "----------------------\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(f'filter[:,:,{i}] = ')\n",
    "    print()\n",
    "    print(full_filter[:,:,i])\n",
    "    print()\n",
    "    print('----------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "256eec6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes_y, indexes_x, indexes_z = np.where(full_filter)\n",
    "indexes = np.array(list(zip(indexes_y, indexes_x, indexes_z)))\n",
    "grouped = np.array(group_by(indexes, lambda index: index[2]))\n",
    "\n",
    "new_indexes = np.array(list(map(indexes_fix, grouped)))\n",
    "new_indexes = np.reshape(new_indexes, (new_indexes.shape[0] * new_indexes.shape[1], 3))\n",
    "\n",
    "new_indexes_y, new_indexes_x, new_indexes_z = np.split(new_indexes, 3, axis=1)\n",
    "new_full_filter = np.zeros(full_filter.shape)\n",
    "new_full_filter[new_indexes_y, new_indexes_x, new_indexes_z] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5750d75c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filter[:,:,0] = \n",
      "\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 1. 1. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 1. 1. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 1. 1. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 1. 1. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      "----------------------\n",
      "filter[:,:,1] = \n",
      "\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 1. 1. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 1. 1. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 1. 1. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 1. 1. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      "----------------------\n",
      "filter[:,:,2] = \n",
      "\n",
      "[[0. 0. 0. 0. 1. 1. 1. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 1. 1. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 1. 1. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 1. 1. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      "----------------------\n",
      "filter[:,:,3] = \n",
      "\n",
      "[[1. 1. 1. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      "----------------------\n",
      "filter[:,:,4] = \n",
      "\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      "----------------------\n"
     ]
    }
   ],
   "source": [
    "full_filter2 = fix_filter2(full_filter)\n",
    "for i in range(5):\n",
    "    print(f'filter[:,:,{i}] = ')\n",
    "    print()\n",
    "    print(full_filter2[:,:,i])\n",
    "    print()\n",
    "    print('----------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "10690d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [45,47,52,66,70,71,74,129,134,146,153,155,160,161,239,248,297,301,329,381,394,794,1700,2007,2009,2092,2174,2179,2180,2260,2263,2265,2266,2269,2437,2439,2595,2762,2845,2923,4379,4384,4386,4390,4418,4459,4489,4492,4500,4545,4548,4552,4564,4568,4575,4580,4582,4585,4628,4632,4638,4641,4648,4654,4669,4723,4727,4736,4739,4744,4748,4756,4762,4812,4816,4821,4824,4831,4883,4892,4898,4910,4913,4916,4918,4966,4989,5007,5013,5057,5076,5083,5094,5097,5140,5154,5155,5168,5171,5183,5223,5224,5227]\n",
    "export_to_matlab('xi.mat', data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data.x(1:10,1:10) = \n",
      "\n",
      "[[ 17  33  49  65  81  97 113 129 145 161]\n",
      " [ 17  33  49  65  81  97 113 129 145 161]\n",
      " [ 17  33  49  65  81  97 113 129 145 161]\n",
      " [ 17  33  49  65  81  97 113 129 145 161]\n",
      " [ 17  33  49  65  81  97 113 129 145 161]\n",
      " [ 17  33  49  65  81  97 113 129 145 161]\n",
      " [ 17  33  49  65  81  97 113 129 145 161]\n",
      " [ 17  33  49  65  81  97 113 129 145 161]\n",
      " [ 17  33  49  65  81  97 113 129 145 161]\n",
      " [ 17  33  49  65  81  97 113 129 145 161]]\n",
      "\n",
      "----------------------\n",
      "data.y(1:10,1:10) = \n",
      "\n",
      "[[ 17  17  17  17  17  17  17  17  17  17]\n",
      " [ 33  33  33  33  33  33  33  33  33  33]\n",
      " [ 49  49  49  49  49  49  49  49  49  49]\n",
      " [ 65  65  65  65  65  65  65  65  65  65]\n",
      " [ 81  81  81  81  81  81  81  81  81  81]\n",
      " [ 97  97  97  97  97  97  97  97  97  97]\n",
      " [113 113 113 113 113 113 113 113 113 113]\n",
      " [129 129 129 129 129 129 129 129 129 129]\n",
      " [145 145 145 145 145 145 145 145 145 145]\n",
      " [161 161 161 161 161 161 161 161 161 161]]\n",
      "\n",
      "----------------------\n",
      "data.u(51:56,73:78) = \n",
      "\n",
      "[[ 0.4162684   0.44626585  0.57322361  2.55313457  1.03327257 -0.45244089]\n",
      " [ 1.76712529  2.84990946  1.65739989  3.66757644 -2.8756672  -3.68445763]\n",
      " [ 4.13279706  5.12879278 -3.58603948 -3.51530694 -7.57882892 -6.55762081]\n",
      " [10.09447491  9.8655339   2.10089531 -6.20255352 -3.67368157 -4.83712197]\n",
      " [ 8.85173804  8.4198591  -0.3102604  -4.25959587 -1.49315428 -1.58454419]\n",
      " [ 0.4668134  -2.74522179 -4.8751502  -4.22802943 -1.88851243 -0.63983483]]\n",
      "\n",
      "----------------------\n",
      "data.v(51:56,73:78) = \n",
      "\n",
      "[[ 0.01195511 -0.1320872   0.01016416  0.05152791  0.06428131 -0.0346374 ]\n",
      " [-0.33468212 -1.32934859 -0.63865469 -0.23371605 -0.18491285 -0.32249097]\n",
      " [-0.93178467 -3.77465797 -2.9270945  -1.8910255  -2.12107598 -1.25696256]\n",
      " [-2.03177635 -3.63268396 -0.8735101  -2.40906638 -6.35193643 -2.80331695]\n",
      " [ 0.69289691 -2.56332134 -0.46069852 -0.32662554 -1.21324781 -0.80208497]\n",
      " [ 3.36019589  0.62722463  1.54800894  2.3324384   3.78987388  1.817585  ]]\n",
      "\n",
      "----------------------\n",
      "data.s2n(51:56,73:78) = \n",
      "\n",
      "[[1.0029496  1.00408316 1.00436926 1.00210083 1.00523293 1.0010078 ]\n",
      " [1.00878906 1.00936162 1.01986325 1.00722873 1.00077379 1.0117234 ]\n",
      " [1.12432063 1.01388884 1.00709188 1.02479339 0.         1.05320573]\n",
      " [1.01429522 1.02466428 1.13585806 1.08092892 1.03846157 1.06870222]\n",
      " [1.08634162 1.1020112  0.         1.00013828 0.         1.00014472]\n",
      " [1.02459335 1.00872743 1.04657614 1.14033163 0.         0.        ]]\n",
      "\n",
      "----------------------\n"
     ]
    }
   ],
   "source": [
    "import scipy.io\n",
    "\n",
    "data = scipy.io.loadmat('../../_data.mat')['data'][0][0]\n",
    "output_data = OutputPIV(x=data[0], y=data[1], u=data[2], v=data[3], s2n=data[4])\n",
    "filtered_data = filter_fields(output_data)\n",
    "\n",
    "print(f'data.x(1:10,1:10) = ')\n",
    "print()\n",
    "print(filtered_data.x[0:10,0:10])\n",
    "print()\n",
    "print('----------------------')\n",
    "\n",
    "print(f'data.y(1:10,1:10) = ')\n",
    "print()\n",
    "print(filtered_data.y[0:10,0:10])\n",
    "print()\n",
    "print('----------------------')\n",
    "\n",
    "print(f'data.u(51:56,73:78) = ')\n",
    "print()\n",
    "print(filtered_data.u[50:56,72:78])\n",
    "print()\n",
    "print('----------------------')\n",
    "\n",
    "print(f'data.v(51:56,73:78) = ')\n",
    "print()\n",
    "print(filtered_data.v[50:56,72:78])\n",
    "print()\n",
    "print('----------------------')\n",
    "\n",
    "print(f'data.s2n(51:56,73:78) = ')\n",
    "print()\n",
    "print(filtered_data.s2n[50:56,72:78])\n",
    "print()\n",
    "print('----------------------')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}